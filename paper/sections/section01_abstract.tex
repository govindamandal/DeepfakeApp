\begin{abstract}
    The rise of deepfake technology has posed significant challenges for digital media authenticity. In this paper, we propose a novel approach for detecting manipulated videos using Vision Transformers (ViTs). Unlike traditional convolutional neural networks (CNNs), ViTs leverage self-attention mechanisms to capture global dependencies within video frames, making them particularly effective in identifying subtle inconsistencies typical of deepfakes. Our method is evaluated on benchmark datasets and shows improved accuracy and robustness compared to state-of-the-art models. This work advances the field of deepfake detection by providing an interpretable, reliable, and adversarial-resistant framework using ViTs, with potential real-world applications to mitigate the spread of deceptive content.
\end{abstract}