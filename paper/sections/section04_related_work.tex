\section{Related Work}
Wang and et al. \cite{df16} presented a Siamese network-based approach in order to make deepfake detection less sensitive to image quality variations. The network leverages both original and degraded images to effectively provide steady segmentation maps of the tampered regions. Besides, the model utilized a Mask-Guided Transformer to model the cooccurrences between manipulated areas and their
neighbors. Their multi-task learning framework amalgamates segmentation, classification, and localization invariance losses. In comparison with state-of-the-art techniques, *LiSiam* accomplished better performance
with 91.44\% AUC on the FaceForensics++ dataset in low-quality settings, with strong generalization for cross-database evaluation.\\\\
A survey on deepfake video detection techniques using deep learning by Athirasree Das and et al. \cite{df15} conducted a comparative study of various deep learning models for detecting deepfake videos. They reviewed techniques such as CNN (ResNet, VGG16, EfficientNet) and RNN (LSTM), highlighting how deep learning methods can capture features that distinguish between real and
fake videos. The authors emphasized that hybrid models combining CNN and RNN solve issues like temporal inconsistencies and provide better detection results. They concluded that CNN models combined with SVM achieved the highest accuracy (98\%) on the DFDC dataset, surpassing CNN-RNN hybrid approaches in some cases.\\\\
Aditya Devasthale et al.\cite{df13} (2022) proposed an adversarially robust method for detecting deepfake videos. They utilized a VGG19-based deep network architecture, enhanced by adversarial training using the Iterative Fast Gradient Sign Method (I-FGSM), to improve the accuracy of deepfake detection. The study achieved notable results in adversarial robustness, demonstrating the model's ability to withstand various white-box adversarial attacks. Their experiments
on the FaceForensic++ dataset showed significant improvements in accuracy compared to non-adversarial models, establishing the effectiveness of the proposed approach.
4. Comparative Analysis of Deepfake Video Detection Using Inception Net and Efficient Net Geetha Rani E et al. (2022) performed a comparative analysis of deepfake video detection techniques using
Inception Net and Efficient Net. The paper emphasizes the effectiveness of convolutional neural networks (CNN) in detecting deepfakes. EfficientNet achieved significantly higher precision than Inception Net, demonstrating the benefit of combining local and global image processing for anomaly detection. The proposed EfficientNetB7 model obtained an accuracy of 83.4\% on the ImageNet dataset, surpassing earlier models in terms of speed and accuracy. Extensive
experiments confirmed that Efficient Net, in combination with Vision Transformers, outperformed traditional CNN models in deepfake detection.\\\\
Kaddar et al. (2021) proposed a deepfake detection method named HCiT, which combines Convolutional
Neural Networks (CNN) with Vision Transformer (ViT). The hybrid model leverages CNN's ability to extract local information and ViT's self-attention mechanism for detecting deepfakes. The HCiT method was tested on the Faceforensics++ and DeepFake Detection Challenge preview datasets, significantly outperforming state-of-the-art methods. The model achieved high generalization across various deepfake generation techniques, showcasing competitive performance, with 96\% accuracy on the Faceforensics++ dataset and 97.82\% on DFDC.\\\\

Doshi et al. (2022) proposed a real-time deepfake detection system using Video Vision Transformer
(ViViT). The system extracts spatio-temporal features from videos and applies a transformer-based architecture for deepfake detection. It was tested on the FaceForensics++ dataset and achieved a training accuracy of 95.8\%, testing accuracy of 93.1\%, and validation accuracy of 93.4\%. Compared to traditional models like ResNet-50 and Xception, ViViT demonstrated superior performance with an inference time of 3.84 ms, making it effective for real-time detection of fake videos.\\\\

Yang Yu and et al. \cite{df09} in their
paper *MSVT: Multiple Spatiotemporal Views Transformer for DeepFake Video Detection*, the authors introduced a novel framework called Multiple Spatiotemporal Views Transformer (MSVT). This framework incorporates two components: the *Local Spatiotemporal View (LSV)* and *Global Spatiotemporal View (GSV)*, designed to capture dynamic inconsistencies in video sequences. For the LSV, consecutive frames are processed to mine local temporal inconsistencies, which are then passed through a temporal transformer and feature fusion module to generate group-level spatiotemporal features. The GSV takes into account the entire video by feeding frame-level features through another temporal transformer and feature fusion module to extract global temporal clues. Finally, a Global-Local Transformer (GLT) is used to integrate both local and global features, leading to the extraction of comprehensive and subtle spatiotemporal clues. Experiments on six large-scale datasets demonstrated significant improvements in DeepFake detection accuracy, showcasing the effectiveness of the MSVT approach.\\\\

In the papaer A Novel Framework Based on a Hybrid Vision Transformer and Deep Neural Network for Deepfake Detection, Shahin et al. (2024) \cite{df02} proposed a hybrid deepfake detection framework combining Vision Transformer (ViT) and Convolutional Autoencoders (CAE) to address the limitations of traditional forgery detection methods. The framework introduces two models: the first integrates ViT with CAE for enhanced image analysis and reconstruction, while the second uses CAE with classical machine learning algorithms to improve feature extraction and classification. The proposed models achieved an accuracy of approximately 87\%, showcasing enhanced performance compared to state-of-the-art techniques.\\\\

Dagar et al. (2023) \cite{df05} proposed a hybrid deepfake video detection model using Xception and LSTM with channel and spatial attention mechanisms (CBAM). The Xception model, employing depthwise separable convolution, captures spatial artifacts, while LSTM handles temporal discrepancies across frames in manipulated videos. The CBAM module refines the feature maps along both spatial and channel dimensions. The model was evaluated on the Div-DF dataset, consisting of face-swap, facial reenactment, and lip-sync manipulations, achieving an accuracy of 93\% and an AUC of 0.98, outperforming several state-of-the-art deepfake detection models.\\\\

Sun et al. (2023)\cite{df06} introduced a multi-scale Convolution-Transformer network, named ConTrans-Detect, for deepfake video detection. This model integrates a multi-scale CNN module for spatial feature extraction using 3D Inception blocks and a multi-branch Transformer module for capturing temporal dynamics. The approach effectively identifies subtle changes in video frames by learning low-level spatial features alongside temporal variations. The model achieved an AUC of 0.929 and an F1 score of 0.920 on the DeepFake Detection Challenge dataset, outperforming several state-of-the-art models while maintaining fewer parameters, highlighting its efficiency and effectiveness.\\\\
Mohsin Albazony et al. (2023)\cite{df08} proposed a novel model for deepfake video detection by combining Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) alongside image preprocessing techniques. The model was evaluated using a dataset containing 135 real videos and 677 fake videos generated through various manipulation tools. Performance was assessed under two scenarios: different dimensions of training data and varying sizes of the training data. The results indicated that the proposed model outperformed previous models, achieving significant improvements in accuracy, precision, recall, and F-Measure, thus demonstrating its effectiveness in distinguishing between real and fake videos.\\\\
In the paper ISTVT: interpretable spatial-temporal video transformer for deepfake detection (ISTVT) (2023), C Zhao el at. \cite{df11} designed to enhance both the performance and interpretability of deepfake detection models. The ISTVT leverages a decomposed spatial-temporal self-attention mechanism alongside a self-subtract mechanism, which improves detection accuracy and robustness over existing video-based deepfake detection approaches, including other video transformers. A key contribution of this work is a novel visualization technique that separates and displays the temporal and spatial features captured by the model, allowing for deeper insights into which features contribute to the prediction process. This interpretability method is
generalizable and can be applied to other video transformers as well. Future research aims to optimize this visualization strategy to further enhance the model's interpretability and effectiveness in detecting deepfakes.