\section{Introduction}
\textbf{Deepfakes} represent manipulated media be it images, videos, or audio—that alter the original context, often creating misleading content that can deceive viewers. Using advanced AI techniques such as Generative Adversarial Networks (GANs), deepfake creators can modify facial expressions, voices, or even simulate entire videos. Deepfake technology, fueled by artificial intelligence (AI), deep learning, and machine learning algorithms, has revolutionized the way we edit and manipulate videos. There are several DeepFake Apps like \textbf{DeepFaceLab}, \textbf{Reface}, \textbf{Deepfakes Web}, with these advanced tools, we can seamlessly perform face swaps, create realistic deepfake videos, and generate high-quality visual content. Notably, these manipulations can be convincing, as demonstrated by viral videos, including fabricated celebrity appearances.\\\\
While deepfakes may serve benign purposes such as satire or entertainment, they also pose significant threats across various domains. From privacy concerns to political manipulation and corporate espionage, deepfakes have the potential to undermine public trust and damage reputations. The rapid improvement in deepfake quality requires equally advanced detection mechanisms to mitigate their spread. Traditional methods for detecting deepfakes rely heavily on convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which focus on local pixel patterns. However, these models may struggle with identifying subtle or widespread inconsistencies across video frames. In contrast, Vision Transformers (ViTs) excel at capturing long-range dependencies by using a self-attention mechanism, making them a promising approach for detecting deepfakes. By processing entire video frames as sequences, ViTs can identify minor artifacts and manipulations more effectively.\\\\
\textbf{Problem Statement}: Current deepfake detection models, while effective in controlled settings, lack the robustness required to detect complex, high-quality deepfakes across diverse datasets. Their inability to generalize to unseen deepfake generation techniques often results in reduced detection accuracy and increased susceptibility to adversarial attacks. Thus, there is a need for more adaptive and scalable detection models that can address these shortcomings.\\\\
In this work, we propose a novel deepfake detection framework based on Vision Transformers (ViTs). ViTs have recently gained attention in the field of computer vision due to their ability to capture long-range dependencies and model global contextual information more effectively than CNNs. By leveraging the strengths of transformers, our model is designed to enhance feature extraction and improve detection accuracy, even in the face of subtle and sophisticated manipulations found in high-fidelity deepfakes.\\\\
The primary contributions of this paper are as follows:
\begin{itemize}
    \item We propose a hybrid model using \textbf{Transformer} for local feature extraction and capturing long-range dependencies in deepfake videos.
    \item We did extensive experiments on the \textbf{FaceForensics++} and \textbf{Celeb-DF} datasets to evaluate the model’s generalization across different deepfake generation techniques.
    \item We provide a comprehensive analysis of our model’s performance, comparing it with state-of-the-art methods to demonstrate its effectiveness in improving both detection accuracy and robustness.
\end{itemize}